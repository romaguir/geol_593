{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b01ce3c",
   "metadata": {},
   "source": [
    "# GEOL 593: Seismology and Earth (*and Mars*) Structure\n",
    "\n",
    "## Lab assignment 10: Using machine learning algorithms to detect and locate earthquakes\n",
    "\n",
    "In this lab, we will learn to use the machine learning-based research tools PhaseNet and GaMMA (Gaussian Mixture Model Associator) to build a seismic catalog. PhaseNet (Zhu et al., 2018) is a deep learning model for picking P and S wave arrivals from continuous seismic data. In order to relate individual travel time picks to earthquakes, phases must be associated with a common origin point (i.e., earthquake hypocenter). We will use GaMMA for the association problem. \n",
    "\n",
    "### 2022 Mauna Loa Eruption\n",
    "On November 27, 11:30 PM HST the Hawaiian volcano Mauna Loa began erupting for the first time since 1984. During the ~2 week eruptive episode, effusive lava flows originating from Mauna Loa's summit caldera covered a total area of > 40 km$^2$. Prior to the eruption, an increased level of seismicity near Mauna Loa was observed, indicating signs of unrest. Additionally, during the eruption, a swarm of seismic activity was recorded, with USGS reporting > 200 events with magnitude greater than 1.2. We will use PhaseNet and GaMMA to further explore the seismicity during the early part of the eruptive sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97270104",
   "metadata": {},
   "source": [
    "### QuakeFlow \n",
    "\n",
    "The end-to-end workflow of detecting and locating earthquakes with machine learning tools is multi-step problem that can get complicated. Luckily for us, researchers have developed tools to make the workflow simpler. In particular, QuakeFlow (Zhu et al., 2023) was recently developed to make it easier to use PhaseNet and GaMMA together to create earthquake catalogs from raw data. For this lab, we will need to set up an environment for QuakeFlow, and install the relevant packages. \n",
    "\n",
    "#### Creating a QuakeFlow environment\n",
    "To avoid conflicting package requirements between QuakeFlow and your default python environment, we will make a specific environment for this project. To do this, open a terminal and enter the following commands.\n",
    "\n",
    "`conda create -n quakeflow`\n",
    "\n",
    "`conda activate quakeflow`\n",
    "\n",
    "`conda install jupyter`\n",
    "\n",
    "#### Installing PhaseNet and GaMMA\n",
    "Both the PhaseNet and GaMMA packages are linked to the github QuakeFlow github repository. Therefore, to download all of the codes you will need, you can do:\n",
    "\n",
    "`git clone --recursive https://github.com/AI4EPS/QuakeFlow`\n",
    "\n",
    "Next, in a terminal, navigate to the directory where you installed QuakeFlow. To change directories in a terminal use `cd` (for both Windows and Unix-based operating systems). Make sure that your anaconda quakeflow environment is activated in the terminal. Once in the QuakeFlow directory, navigate into the PhaseNet directory, and install the package with `pip install -e .`. Do the same for the GaMMA package. Let me know if you have trouble with any of these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e57128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import obspy\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "from collections import defaultdict\n",
    "from obspy import UTCDateTime\n",
    "from obspy.clients.fdsn import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d092c6",
   "metadata": {},
   "source": [
    "###  <font color='red'>Question 1 </font>\n",
    "\n",
    "To keep track of all of the parameters required to generate a seismic catalog, we will make a 'configuration' file, in the 'json' format. The cell below provides a function where you can set all of the important parameters, including the geographical extent of your study region, the seismic network/ stations you would like to analyze, and the time period you are interested in. In this study, we would like to use broadband stations in the PT network, during the beginning of the eruptive sequence.\n",
    "\n",
    "**Modify the `set_config` function so that the time period starts 1 hour prior to the eruption, and ends 3 hours after the start of the eruption.** To do this, you will need to set the `starttime` and `duration_s` parameters. Remember, the time reported above for the start of the eruption is in HST, and you want UTC!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465083bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_config(config_json='config.json'):\n",
    "    region_name = \"Hawaii\"\n",
    "    \n",
    "    center = (-155.5, 19.5) #longitude, latitude \n",
    "    horizontal_degree = 1.0\n",
    "    vertical_degree = 1.0\n",
    "\n",
    "    #-----complete these parameters\n",
    "    #starttime = obspy.UTCDateTime('')\n",
    "    #duration_hrs = \n",
    "    #duration_s = \n",
    "    \n",
    "    endtime = starttime + duration_s\n",
    "    client = \"IRIS\"\n",
    "    network_list = [\"PT\"]\n",
    "    channel_list = 'HHZ'\n",
    "    \n",
    "    ####### save config ########\n",
    "    degree2km = np.pi * 6371 / 180\n",
    "    config = {}\n",
    "    config[\"region\"] = region_name\n",
    "    config[\"center\"] = center\n",
    "    config[\"xlim_degree\"] = [\n",
    "        center[0] - horizontal_degree / 2,\n",
    "        center[0] + horizontal_degree / 2,\n",
    "    ]\n",
    "    config[\"ylim_degree\"] = [\n",
    "        center[1] - vertical_degree / 2,\n",
    "        center[1] + vertical_degree / 2,\n",
    "    ]\n",
    "    config[\"min_longitude\"] = center[0] - horizontal_degree / 2\n",
    "    config[\"max_longitude\"] = center[0] + horizontal_degree / 2\n",
    "    config[\"min_latitude\"] = center[1] - vertical_degree / 2\n",
    "    config[\"max_latitude\"] = center[1] + vertical_degree / 2\n",
    "    config[\"degree2km\"] = degree2km\n",
    "    config[\"starttime\"] = starttime.datetime.isoformat(timespec=\"milliseconds\")\n",
    "    config[\"endtime\"] = endtime.datetime.isoformat(timespec=\"milliseconds\")\n",
    "    config[\"networks\"] = network_list\n",
    "    config[\"channels\"] = channel_list\n",
    "    config[\"client\"] = client\n",
    "\n",
    "    ## PhaseNet\n",
    "    config[\"phasenet\"] = {}\n",
    "    ## GaMMA\n",
    "    config[\"gamma\"] = {}\n",
    "    ## HypoDD\n",
    "    config[\"hypodd\"] = {\"MAXEVENT\": 1e4}\n",
    "\n",
    "    with open(config_json, \"w\") as fp:\n",
    "        json.dump(config, fp, indent=2)\n",
    "\n",
    "    print(json.dumps(config, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e8b731",
   "metadata": {},
   "source": [
    "###  <font color='red'>Question 2 </font>\n",
    "\n",
    "Run the cell below to create the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d17d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run configuration set up\n",
    "set_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbf81ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_stations(config_json='config.json',plot=True):\n",
    "\n",
    "    client = Client(\"IRIS\")\n",
    "    fname_list = [\"fname\"]\n",
    "    \n",
    "    with open(config_json, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "        \n",
    "    #make a directory for this project (if it doesn't exist)\n",
    "    base_dir = '{}'.format(config['region'])\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)\n",
    "         \n",
    "    ####### Download stations ########\n",
    "    stations = client.get_stations(\n",
    "        network=\",\".join(config[\"networks\"]),\n",
    "        station=\"*\",\n",
    "        starttime=config[\"starttime\"],\n",
    "        endtime=config[\"endtime\"],\n",
    "        minlongitude=config[\"xlim_degree\"][0],\n",
    "        maxlongitude=config[\"xlim_degree\"][1],\n",
    "        minlatitude=config[\"ylim_degree\"][0],\n",
    "        maxlatitude=config[\"ylim_degree\"][1],\n",
    "        channel=config[\"channels\"],\n",
    "        level=\"response\",\n",
    "        # filename=\"stations.xml\"\n",
    "    )\n",
    "\n",
    "    ####### Save stations ########\n",
    "    station_locs = defaultdict(dict)\n",
    "    for network in stations:\n",
    "        for station in network:\n",
    "            print(station)\n",
    "            for chn in station:\n",
    "                \n",
    "                #print(chn)\n",
    "                print(chn.response)\n",
    "                \n",
    "                sid = f\"{network.code}.{station.code}.{chn.location_code}.{chn.code[:-1]}\"\n",
    "                if sid in station_locs:\n",
    "                    if chn.code[-1] not in station_locs[sid][\"component\"]:\n",
    "                        station_locs[sid][\"component\"].append(chn.code[-1])\n",
    "                        station_locs[sid][\"response\"].append(round(chn.response.instrument_sensitivity.value, 2))\n",
    "                else:\n",
    "                    tmp_dict = {\n",
    "                        \"longitude\": chn.longitude,\n",
    "                        \"latitude\": chn.latitude,\n",
    "                        \"elevation(m)\": chn.elevation,\n",
    "                        \"component\": [\n",
    "                            chn.code[-1],\n",
    "                        ],\n",
    "                        \"response\": [\n",
    "                            round(chn.response.instrument_sensitivity.value, 2),\n",
    "                        ],\n",
    "                        \"unit\": chn.response.instrument_sensitivity.input_units.lower(),\n",
    "                        #\"unit\": 'm/s',\n",
    "                    }\n",
    "                    station_locs[sid] = tmp_dict\n",
    "                    \n",
    "    stations.write('{}/stations.xml'.format(base_dir),format='STATIONXML')\n",
    "\n",
    "    station_json = '{}/stations.json'.format(base_dir)\n",
    "    with open(station_json, \"w\") as fp:\n",
    "        json.dump(station_locs, fp, indent=2)\n",
    "\n",
    "    station_pkl = '{}/stations.pkl'.format(base_dir)\n",
    "    with open(station_pkl, \"wb\") as fp:\n",
    "        pickle.dump(stations, fp)\n",
    "    \n",
    "    if plot:\n",
    "        ######## Plot stations ########\n",
    "        station_locs = pd.DataFrame.from_dict(station_locs, orient=\"index\")\n",
    "        plt.figure()\n",
    "        plt.plot(station_locs[\"longitude\"], station_locs[\"latitude\"], \"^\", label=\"Stations\")\n",
    "        plt.xlabel(\"X (km)\")\n",
    "        plt.ylabel(\"Y (km)\")\n",
    "        plt.axis(\"scaled\")\n",
    "        plt.legend()\n",
    "        plt.title(f\"Number of stations: {len(station_locs)}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de97812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_waveforms(config_json='config.json'):\n",
    "\n",
    "    client = Client(\"IRIS\")\n",
    "    fname_list = [\"fname\"]\n",
    "    \n",
    "    with open(config_json, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "        \n",
    "    waveform_dir = '{}/{}'.format(config['region'],'waveforms')\n",
    "    \n",
    "    if not os.path.exists(waveform_dir):\n",
    "        os.makedirs(waveform_dir)\n",
    "       \n",
    "    station_pkl = '{}/stations.pkl'.format(config['region'])\n",
    "    with open(station_pkl, \"rb\") as fp:\n",
    "        stations = pickle.load(fp)\n",
    "\n",
    "        \n",
    "    #loop through stations and download data\n",
    "    #-----------------------------------------------------------------------------\n",
    "    max_retry = 10\n",
    "    stream = obspy.Stream()\n",
    "    \n",
    "    starttime = UTCDateTime(config['starttime'])\n",
    "    endtime = UTCDateTime(config['endtime'])\n",
    "    \n",
    "    num_sta = 0\n",
    "    for network in stations:\n",
    "        for station in network:\n",
    "            print(f\"********{network.code}.{station.code}********\")\n",
    "            \n",
    "            retry = 0\n",
    "            while retry < max_retry:\n",
    "                try:\n",
    "                    tmp = client.get_waveforms(\n",
    "                        network.code,\n",
    "                        station.code,\n",
    "                        \"*\",\n",
    "                        config[\"channels\"],\n",
    "                        starttime,\n",
    "                        endtime,\n",
    "                        )\n",
    "                    stream += tmp\n",
    "                    num_sta += len(tmp)\n",
    "                    break\n",
    "                except Exception as err:\n",
    "                    print(\"Error {}.{}: {}\".format(network.code, station.code, err))\n",
    "                    message = \"No data available for request.\"\n",
    "                    if str(err)[: len(message)] == message:\n",
    "                        break\n",
    "                    retry += 1\n",
    "                    time.sleep(5)\n",
    "                    continue\n",
    "            if retry == max_retry:\n",
    "                print(f\"{fname}: MAX {max_retry} retries reached : {network.code}.{station.code}\")\n",
    "   \n",
    "    #-----------------------------------------------------------------------------   \n",
    "    \n",
    "    fname = \"{}.mseed\".format(starttime.datetime.strftime(\"%Y-%m-%dT%H:%M:%S\"))\n",
    "    \n",
    "    if len(stream) > 0:\n",
    "        stream.write(os.path.join(waveform_dir, fname))\n",
    "        print('download successful')\n",
    "    else:\n",
    "        print('download failed')\n",
    "        \n",
    "    fname_list.append(fname)\n",
    "    fname_csv = 'input_data.csv'\n",
    "    with open(fname_csv, \"w\") as fp:\n",
    "        fp.write(\"\\n\".join(fname_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914c6def",
   "metadata": {},
   "source": [
    "###  <font color='red'>Question 3 </font>\n",
    "\n",
    "The next step is to download the station information and waveforms. The two code blocks above provide the functions `download_stations` and `download_waveforms`, which read the parameters set in the `config.json` file. In the block below, run both of these functions. Note, the waveforms and station data will be saved in the directory `./Hawaii/` (or whatever you set your `region_name` to in `set_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273a71f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer Q3 here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e073a8",
   "metadata": {},
   "source": [
    "###  <font color='red'>Question 4 </font>\n",
    "\n",
    "**Make a map of your stations with cartopy**. The station metadata (including latitude & longitude) are written in the file `Hawaii/stations.json`. In the box below, I have written code to read this file, which loads it into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49976dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer Q4 here.\n",
    "import cartopy\n",
    "\n",
    "with open('Hawaii/stations.json', \"r\") as fp:\n",
    "    station_info = json.load(fp)\n",
    "    \n",
    "#complete code to plot stations map below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9c51ab",
   "metadata": {},
   "source": [
    "###  <font color='red'>Question 5 </font>\n",
    "\n",
    "Make a plot of the waveforms that you downloaded. You can load the data stream with `obspy.read()`, and plot it with `st.plot()`. The data was written to a mseed file in `Hawaii/waveforms/`.\n",
    "\n",
    "Hint: to see the name of the mseed file from jupyter, you can type `ls Hawaii/waveforms` (i.e., to list the contents of that directory). `ls` is a linux command, but most linux commands work when run in jupyter notebook code cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03e2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer Q5 here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfeb571",
   "metadata": {},
   "source": [
    "###  <font color='red'>Question 6 </font>\n",
    "\n",
    "To detect P and S waves in the data from each station, we will run PhaseNet's `predict.py` script. This is meant to be run from the command line, but we can run it within a jupyter notebook cell also. We just need to set some paths, including where the `predict.py` script is located, what model we would like use, and where our data is located.\n",
    "\n",
    "**The block below is set up to run PhaseNet on my machine. Modify the code to run on your machine, by changing `predict_path` and `model_path` to the corresponding paths on your machine. Then run the code block!**\n",
    "\n",
    "Note, if you are successful, you should see some code output, and a message that looks something like \"Done with XXX P-picks and YYY S-picks\". It may take a minute or two. Take note of how many P and S waves were picked by PhaseNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cd7acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_path='/Users/rmaguire/Tools/QuakeFlow/PhaseNet/phasenet/predict.py'\n",
    "model_path='/Users/rmaguire/Tools/QuakeFlow/PhaseNet/model/190703-214543/'\n",
    "\n",
    "command = \"python {} --model={} --data_dir=Hawaii/waveforms/ \\\n",
    "--data_list=input_data.csv --stations=Hawaii/stations.json \\\n",
    "--result_dir='./Hawaii' --format=mseed_array --amplitude\".format(predict_path,model_path)\n",
    "\n",
    "#run this as a command line argument\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb97cfc",
   "metadata": {},
   "source": [
    "###  <font color='red'>Question 7 </font>\n",
    "\n",
    "If successful, PhaseNet should have created a file in the `Hawaii` directory called `picks.csv`, which contains a table of all of the potential P and S wave arrivals detected at each station. This file will serve as one of the inputs to GaMMA. To run the earthquake association problem and create a catalog of seismic events, run the block below.\n",
    "\n",
    "This step can take several minutes. For ~1000 P-wave or S-wave picks, it runs in ~5 minutes on my laptop. If you have many more than 1000 picks from PhaseNet, you may have set some parameters incorrectly (e.g., the period of time is too long, or you included too many stations), and you should come to me for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8854fb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pyproj import Proj\n",
    "from gamma.utils import association\n",
    "\n",
    "pick_csv = 'Hawaii/picks.csv'\n",
    "station_json = 'Hawaii/stations.json'\n",
    "config_json = 'config.json'\n",
    "\n",
    "picks = pd.read_csv(pick_csv, parse_dates=[\"phase_time\"])\n",
    "picks[\"id\"] = picks[\"station_id\"]\n",
    "picks[\"timestamp\"] = picks[\"phase_time\"]\n",
    "picks[\"amp\"] = picks[\"phase_amp\"]\n",
    "picks[\"type\"] = picks[\"phase_type\"]\n",
    "picks[\"prob\"] = picks[\"phase_score\"]\n",
    "\n",
    "with open(config_json, \"r\") as fp:\n",
    "    config = json.load(fp)\n",
    "\n",
    "with open(station_json, \"r\") as fp:\n",
    "    stations = json.load(fp)\n",
    "    stations = pd.DataFrame.from_dict(stations, orient=\"index\")\n",
    "    stations[\"id\"] = stations.index\n",
    "    proj = Proj(f\"+proj=sterea +lon_0={config['center'][0]} +lat_0={config['center'][1]} +units=km\")\n",
    "    stations[[\"x(km)\", \"y(km)\"]] = stations.apply(\n",
    "        lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1)\n",
    "    stations[\"z(km)\"] = stations[\"elevation(m)\"].apply(lambda x: -x / 1e3)\n",
    "    \n",
    "## setting GMMA configs\n",
    "config[\"use_dbscan\"] = False\n",
    "config[\"use_amplitude\"] = True\n",
    "config[\"method\"] = \"BGMM\"\n",
    "if config[\"method\"] == \"BGMM\":  ## BayesianGaussianMixture\n",
    "    config[\"oversample_factor\"] = 4\n",
    "if config[\"method\"] == \"GMM\":  ## GaussianMixture\n",
    "    config[\"oversample_factor\"] = 1\n",
    "\n",
    "# Earthquake location\n",
    "config[\"dims\"] = [\"x(km)\", \"y(km)\", \"z(km)\"]\n",
    "config[\"vel\"] = {\"p\": 6.0, \"s\": 6.0 / 1.73}\n",
    "config[\"x(km)\"] = (np.array(config[\"xlim_degree\"]) - np.array(config[\"center\"][0])) * config[\"degree2km\"]\n",
    "config[\"y(km)\"] = (np.array(config[\"ylim_degree\"]) - np.array(config[\"center\"][1])) * config[\"degree2km\"]\n",
    "config[\"z(km)\"] = (0, 60)\n",
    "config[\"bfgs_bounds\"] = (\n",
    "    (config[\"x(km)\"][0] - 1, config[\"x(km)\"][1] + 1),  # x\n",
    "    (config[\"y(km)\"][0] - 1, config[\"y(km)\"][1] + 1),  # y\n",
    "    (0, config[\"z(km)\"][1] + 1),  # z\n",
    "    (None, None),  # t\n",
    "    )\n",
    "\n",
    "# DBSCAN\n",
    "config[\"dbscan_eps\"] = 10  # second\n",
    "config[\"dbscan_min_samples\"] = 3  ## see DBSCAN\n",
    "\n",
    "# Filtering\n",
    "print(stations)\n",
    "config[\"min_picks_per_eq\"] = min(10, len(stations) // 3)\n",
    "config[\"min_p_picks_per_eq\"] = 0\n",
    "config[\"min_s_picks_per_eq\"] = 0\n",
    "config[\"max_sigma11\"] = 2.0  # s\n",
    "config[\"max_sigma22\"] = 2.0  # m/s\n",
    "config[\"max_sigma12\"] = 1.0  # covariance\n",
    "\n",
    "# if use amplitude\n",
    "if config[\"use_amplitude\"]:\n",
    "    picks = picks[picks[\"amp\"] != -1]\n",
    "\n",
    "# print(config)\n",
    "event_idx0 = 1\n",
    "assignments = []\n",
    "for k, v in config.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "    \n",
    "#Run GaMMA association\n",
    "catalogs, assignments = association(picks, stations, config, event_idx0, method=config[\"method\"])\n",
    "event_idx0 += len(catalogs)\n",
    "\n",
    "## create catalog--------------------------------------------------------------------------------\n",
    "catalogs = pd.DataFrame(\n",
    "    catalogs,\n",
    "    columns=[\"time\"]\n",
    "    + config[\"dims\"]\n",
    "    + [\n",
    "        \"magnitude\",\n",
    "        \"sigma_time\",\n",
    "        \"sigma_amp\",\n",
    "        \"cov_time_amp\",\n",
    "        \"event_index\",\n",
    "        \"gamma_score\",\n",
    "    ],\n",
    "    )\n",
    "\n",
    "catalogs[[\"longitude\", \"latitude\"]] = catalogs.apply(\n",
    "    lambda x: pd.Series(proj(longitude=x[\"x(km)\"], latitude=x[\"y(km)\"], inverse=True)),\n",
    "    axis=1,\n",
    "    )\n",
    "catalogs[\"depth(m)\"] = catalogs[\"z(km)\"].apply(lambda x: x * 1e3)\n",
    "\n",
    "gamma_catalog_csv = 'gamma_catalog.csv'\n",
    "catalogs.sort_values(by=[\"time\"], inplace=True)\n",
    "with open(gamma_catalog_csv, \"w\") as fp:\n",
    "    catalogs.to_csv(\n",
    "            fp,\n",
    "            # sep=\"\\t\",\n",
    "            index=False,\n",
    "            float_format=\"%.3f\",\n",
    "            date_format=\"%Y-%m-%dT%H:%M:%S.%f\",\n",
    "            columns=[\n",
    "                \"time\",\n",
    "                \"magnitude\",\n",
    "                \"longitude\",\n",
    "                \"latitude\",\n",
    "                \"depth(m)\",\n",
    "                \"sigma_time\",\n",
    "                \"sigma_amp\",\n",
    "                \"cov_time_amp\",\n",
    "                \"gamma_score\",\n",
    "                \"event_index\",\n",
    "            ],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc06d0c",
   "metadata": {},
   "source": [
    "###  <font color='red'>Question 8 </font>\n",
    "\n",
    "The above code block ran GaMMA and created the earthquake catalog `gamma_catalog.csv`. Let's take a look at what we found!\n",
    "\n",
    "First, read in the catalog to a pandas dataframe, with `df = pd.read_csv`. How many earthquakes did you find?\n",
    "\n",
    "Next, make a plot to summarize the earthquake statistics. Make a figure with 2 axes, that include histograms of the event magnitudes and event depths. You can do this with `plt.hist` (or `ax.hist` if you are using an axes object). Be sure to label axes. \n",
    "\n",
    "Hint: You can use the pandas dataframe in a similar way to a dictionary. For example, to select the array of magnitudes, you can use df['magnitude']. To see all of the variables in the catalog, you can type `df.keys()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56f915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer Q8 here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a0a16d",
   "metadata": {},
   "source": [
    "###  <font color='red'>Question 9 </font>\n",
    "**Use cartopy to make a map of your seismicity catalog.**. To make things more interesting, instead of plotting simple scatter points, color the points by the event depth (and include a colorbar). You can color the points by setting the argument `c` in `ax.scatter()` to the array of event depths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820c07ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer Question 9 here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05da96e",
   "metadata": {},
   "source": [
    "###  <font color='red'>Question 10 </font>\n",
    "Lastly, visit the USGS Earthquake Map (https://earthquake.usgs.gov/earthquakes/map/), and compare to see how your catalog looks with the USGS's. To search the catalog, click the cog-wheel icon in the top right-hand corner of the page, and scroll until you see the button labeled \"SEARCH EARTHQUAKE CATALOG\". This will allow you to perform a custom query. Your query should include events of all magnitudes during the same time frame as your analysis (i.e., starting 1 hour prior to eruption and ending 3 hours after the beggining of the eruption). To set the region, use the \"Draw Rectangle on Map\" feature, and draw a box around the big island of Hawaii. \n",
    "\n",
    "What differences do you notice? How many earthquakes are in the catalog compared to yours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b76c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer Q10 here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
