{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d77885e",
   "metadata": {},
   "source": [
    "# GEOL 593: Seismology and Earth Structure\n",
    "\n",
    "## Lab assignment 5: Surface wave inversion\n",
    "\n",
    "In today's lab we will explore how surface waves can be used to constrain the seismic velocity structure of the Earth's interior. In particular, we will learn how to measure **group velocity dispersion** of Rayleigh waves, and use a probabilistic inversion to estimate the seismic velocity structure in the crust and uppermost mantle. This is a particularly powerful method because it allows us to estimate a path-averaged seismic velocity profile using recordings of an earthquake at a single seismic station. \n",
    "\n",
    "The earthquake that we will analyze occurred on December 20, 2022 near the coast of Northern California, and resulted from strike-slip faulting. The hypocenter was located at a depth of 17.9 km and the event was a magnitude Mw 6.4. More information can be found on the IRIS event page here: https://ds.iris.edu/ds/nodes/dmc/tools/event/11635701. The event was clearly visible on broadband stations throughout the US. In this lab, we will look at waveforms from the station IU.DWPF, located in Disney Wilderness Preserve in Florida. By measuring Rayleigh wave group velocity dispersion from this event measured at IU.DWPF, we will be able to constrain the average shear wave speed structure between California and Florida.\n",
    "\n",
    "### Package requirements\n",
    "\n",
    "In addition to obspy and cartopy (which you have used previously), this lab requires two additional packages. The first package is `disba` (https://github.com/keurfonluu/disba), which will allow us to predict surface wave dispersion curves for different Earth models. The second package is `splipy` (https://pypi.org/project/Splipy/), which is a python library for dealing with B-spline functions. Both of these packages will be used for the probabilistic inversion of surface wave dispersion curves. They can both be installed with the package manager **pip** by typing the following in a terminal\n",
    "\n",
    "`pip install disba` \\\n",
    "`pip install splipy`\n",
    "\n",
    "If you have difficulty installing either of these packages, please let me know!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb5c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import disba\n",
    "import cartopy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from obspy import UTCDateTime\n",
    "from obspy.clients.fdsn import Client\n",
    "from disba import GroupDispersion\n",
    "from splipy import BSplineBasis\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f4e82e",
   "metadata": {},
   "source": [
    "### Downloading data\n",
    "\n",
    "Run the cell below to download the data for the Mw 6.4 N. California event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f33f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize IRIS client\n",
    "client = Client(\"IRIS\") \n",
    "\n",
    "#Indonesia event\n",
    "origin = UTCDateTime('2022-12-20 10:34:24') #origin time of event\n",
    "evlo = -124.42 #event longitude\n",
    "evla = 40.525 #event latitude\n",
    "\n",
    "starttime = origin\n",
    "endtime = starttime + 60.*40 # we want 40 minutes of seismic data, starting at the origin time\n",
    "st = client.get_waveforms(\"IU\",\"DWPF\", \"00\", \"BHZ\", starttime, endtime,attach_response=True) #download waveforms\n",
    "st.remove_response(output='DISP',water_level=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549c85ac",
   "metadata": {},
   "source": [
    "### <font color='red'>Question 1 </font> \n",
    "After running the block above, you have the vertical component waveform data in *displacement* (in the Obspy stream `st`). Plot the displacement waveform using matplotlib. The x-axis should be in units of seconds and the y axis should be in units of meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d830c9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer Q1 here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d207d5",
   "metadata": {},
   "source": [
    "### <font color='red'>Question 2 </font> \n",
    "\n",
    "The station IU.DWPF is located in the Disney Wilderness Preserve in Florida, with coordinates Latitude: 28.11, Longitude: -81.43. The earthquake (as shown above) has coordinates Latitude: 40.52, Longitude: -124.42. \n",
    "\n",
    "i) Use cartopy to plot a map of the earthquake and seismometer.\n",
    "\n",
    "ii) Using the function `obspy.geodetics.gps2dist_azimuth`, calculate the epicentral distance of the event, in km.\n",
    "\n",
    "Hint: The function `gps2dist_azimuth` takes 4 arguments; (lat1,lon1,lat2,lon2), where lat1 and lon1 are the coordinates of the earthquake and lat2 and lon2 are the coordinates of the seismic station. The function returns a python **list**. The first item of the list is the distance **in m**. See the documentation here: https://docs.obspy.org/packages/autogen/obspy.geodetics.base.gps2dist_azimuth.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6a4a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer Question 2 here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4fdd13",
   "metadata": {},
   "source": [
    "### <font color='red'>Question 3 </font> \n",
    "\n",
    "Make a copy of your stream (e.g., `stc = st.copy()`), and bandpass filter it between 40 - 50 s (e.g., `stc.filter('bandpass',freqmin=1./50,freqmax=1/40.,corners=2,zerophase=True)`. Plot the filtered waveform, along with it's envelope. Hint; remember from lab 2 we can plot the envelope using the Hilbert transform; `scipy.signal.hilbert`)\n",
    "\n",
    "At what time (in seconds) do you observe the peak of the envelope of the waveform? The time can be approximate (i.e., it's ok to eyeball it here). We will measure the dispersion more accurately next. Based on your calculation of event distance above, what group velocity does the peak of the envelope correspond to? Hint, the group velocity is calculated by dividing the distance of the event by the time that corresponds to the peak of the envelope function of the bandpass filtered signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada49f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer Q3 here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160e2051",
   "metadata": {},
   "source": [
    "### Measuring surface wave dispersion\n",
    "\n",
    "The analysis you performed above is essentially how we measure surface wave group velocity. However, you performed the analysis using just one bandpass filter, so you only calculated one group velocity measurement (in this case for waves period between ~40 - 50 s). What we are most interested in with surface waves is the dependency of group velocity on frequency (i.e., the dispersion). To calculate the 'dispersion curve', we need to repeat the process above but for a range of frequency bands. Below, a function `calc_disp_curve` is provided to do this.\n",
    "\n",
    "The function takes two arguments; i) the obspy data stream, and ii) the epicentral distance in km. If you run the function it will produce a Rayleigh wave group velocity dispersion plot. The plot shows period on the x-axis against group velocity (and time) on the y-axis. Each column of the plot is colored by the values of the envelope of the Rayleigh wave signal filtered at the corresponding period. The group velocity dispserion curve is found by finding the peak value (the brightest color) at each period. It can be thought of as following a 'ridge' of high values in the color plot. The measured dispersion curve is plotted as blue scatter points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66851bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dispersion(st,dist_km):\n",
    "    '''\n",
    "    creates a dispersion plot, provided a stream object (with only 1 trace), and an epicentral distance (in km)\n",
    "    '''\n",
    "    fig,ax = plt.subplots(figsize=[6,6])\n",
    "    periods = np.linspace(15,50,36)\n",
    "    time = st[0].times()\n",
    "    disp_plot = []\n",
    "    t_max = []\n",
    "    \n",
    "    for period in periods:\n",
    "        period_min = period - 5.\n",
    "        period_max = period + 5.\n",
    "        freq_min = 1./period_max\n",
    "        freq_max = 1./period_min\n",
    "        stc = st.copy()\n",
    "        stc.filter('bandpass',freqmin=freq_min,freqmax=freq_max,corners=4,zerophase=True)\n",
    "        env = np.abs(hilbert(stc[0].data))\n",
    "        env /= np.max(env)\n",
    "        ind = np.argmax(env)\n",
    "        t_max.append(time[ind])\n",
    "        disp_plot.append(env)\n",
    "    \n",
    "    disp_plot = np.array(disp_plot)\n",
    "    #plt.imshow(disp_plot,aspect='auto',cmap='magma')\n",
    "\n",
    "    ax.pcolormesh(periods,time,disp_plot.T,cmap='magma')\n",
    "    ax.set_ylim([1800,700])\n",
    "    \n",
    "    #functions for mapping time axis to group velocity axis\n",
    "    def time_to_vel(x,dist_km=dist_km):\n",
    "        vel = dist_km/x\n",
    "        return vel\n",
    "    def vel_to_time(x,dist_km=dist_km):\n",
    "        time = dist_km/x\n",
    "        return time\n",
    "    \n",
    "    #make secondary axis in group velocity\n",
    "    gvel_ax = ax.secondary_yaxis('right', functions=(time_to_vel,vel_to_time))\n",
    "    \n",
    "    #convert t_max to group velocity\n",
    "    t_max = np.array(t_max)\n",
    "    group_vel = dist_km / t_max\n",
    "    \n",
    "    ax.set_xlabel('period (s)')\n",
    "    ax.set_ylabel('time (s)')\n",
    "    gvel_ax.set_ylabel('group velocity (km/s)')\n",
    "    ax.scatter(periods,t_max)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ee2e5",
   "metadata": {},
   "source": [
    "### <font color='red'>Question 4 </font> \n",
    "\n",
    "Modify the function above so that it returns both the group velocity dispersion curve and the periods at which they were measured. In other words, when you run the function, it should make the dispersion plot, and return two vectors which correspond to the scatter points shown in the plot.\n",
    "\n",
    "Hint: When after you modify the function, calling the function should look something like this\\\n",
    "`per_observed, vel_observed = calc_dispserion(st,dist_km)` \\\n",
    "where `per_observed` and `vel_observed` are arrays corresponding to the periods and group velocities that were measured (i.e., the scatter points shown in the plot).\n",
    "\n",
    "After modifying the function, run it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0326194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer Question 4 here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62225570",
   "metadata": {},
   "source": [
    "### Inverting a dispersion curve for seismic velocity structure\n",
    "\n",
    "The dispersion curve we just measured provides powerful constraints on the seismic velocity structure between the earthquake and the station. In the period range of 15 - 50 s, the waves are mostly sensitive to the crust and uppermost mantle. Our next step is to invert the dispersion curve for the average velocity-depth profile along the path of propagation. Unfortunately, because the 'sensitivity kernels' of surface waves (i.e., the regions in the subsurface to which surface waves are most sensitive) depend on the seismic wave speed structure we are trying to find, this problem is **non-linear**. In other words, we can not set this problem up as a matrix vector system $\\mathbf{G}\\mathbf{m} = \\mathbf{d}$, as we have done with previous problems.\n",
    "\n",
    "There are a variety of methods to solve non-linear problems, but here we are going to take a probabalistic approach based on *Bayes Theorem*. Bayes theorem relates the probability $p$ of a model $m$ given a dataset $d$ (written $p(m|d)$) to the probability of observing our dataset provided a model (written $p(d|m)$):\n",
    "\n",
    "$p(m|d) \\propto p(m)p(d|m)$\n",
    "\n",
    "where $p(m|d)$ is called the *posterior* (the probability distribution we would like to find), $p(m)$ is called the *prior* (the constraints we have on the model from apriori information), and $p(d|m)$ is called the *likelihood function*. The likelihood function is defined as\n",
    "\n",
    "$p(d|m) = \\exp\\left(\\frac{-\\phi(m)}{2}\\right)$\n",
    "\n",
    "where $\\phi(m)$ is the misfit function based on the difference between our observations and the predictions for a given Earth model. We will use a least squares misfit function \n",
    "\n",
    "$\\phi(m) = \\lVert \\frac{g(m) - d^2}{\\sigma_d} \\rVert$\n",
    "\n",
    "Here, $g(m)$ corresponds to the *forward problem* (i.e., predicting the dispersion curve for a model $m$), $d$ is the observed dispersion curve, and $\\sigma_d$ is the observational error (i.e., the standard error of the group velocity dispersion measurements). The data error $\\sigma_d$ can be difficult to know exactly, but here we will assume a value of 0.1 km/s. \n",
    "\n",
    "Practically, to estimate the posterior probability disribution $p(m|d)$ we will probabilistically sample the *model space* (i.e., the range of plausible model parameters) using a *Markov-chain Monte Carlo* (MCMC) approach (https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo). Below, the function `run_mcmc` is given, which provides a crude MCMC algorithm. The aglorithm iteratively samples the model space by randomly selecting an Earth model (i.e., a seismic velocity structure) from a prior disribution. In our case, the prior distribution gives a range of reasonable seismic velocity values in the crust. The dispersion curve for this model is predicted (using the disba package), and the model is either accepted or rejected based on the misfit between the predicted and observed group velocity dispersion. If the model is accepted, it is added to a 'chain' of plausible models that fit the data reasonably well. After all MCMC iterations are complete, the chain of accepted models is used to approximate the posterior probability disribution $p(m|d)$. \n",
    "\n",
    "In the code block below, find two functions that are used to perform the inversion. The first function, `gen_model`, is a helper function to generate a model based on 'proposed' parameters (i.e., parameters drawn from our prior). The second function, `run_mcmc`, is used to run the MCMC inversion. There are three parameters that you must provide, detailed below:\n",
    "\n",
    "    per_observed = an array of periods at which group velocity measurements were made (in s)\n",
    "    vel_observed = an arary of group velocity measurements, made at each perion in 'per_observed'\n",
    "    iterations = number of iterations to run the inversion for\n",
    "    \n",
    "Running `run_mcmc` will return three things:\n",
    "\n",
    "    accepted = a list of accepted models\n",
    "    misfits = a list of the misfits corresponding to the models in 'accepted'\n",
    "    best_model = the model with the lowest misfit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3f8f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_model(moho,coefs,vs_mantle):\n",
    "    '''\n",
    "    -------------------------------------------------------------------\n",
    "    input parameters:\n",
    "    -------------------------------------------------------------------\n",
    "    moho =  the proposed moho depth (ie. crustal thickness), given in km\n",
    "    coefs =  an array of proposed spline coefficients that describe the crustal shear velocity structure\n",
    "    vs_mantle =  the proposed shear velocity in the mantle\n",
    "    \n",
    "    -------------------------------------------------------------------\n",
    "    returns:\n",
    "    -------------------------------------------------------------------\n",
    "    vel_model = an array containing the velocity and density of the proposed model, in a format that 'disba' can read\n",
    "    '''\n",
    "    #fixed for 5 splines\n",
    "    order = 3\n",
    "    knots = np.array([0,0,0,0.35,0.7,1,1,1])\n",
    "    \n",
    "    #be careful changing the number of points. More layers means disba takes more time to calculate a dispersion curve.\n",
    "    npts_crust = 20\n",
    "    npts_mantle = 4\n",
    "    \n",
    "    crust = np.linspace(0,moho,npts_crust)\n",
    "    mantle = np.linspace(moho,100,npts_mantle)\n",
    "    depth = np.hstack((crust,mantle))\n",
    "\n",
    "    knots *= np.max(crust)\n",
    "    bs = BSplineBasis(knots=knots,order=order)\n",
    "    basis = bs.evaluate(crust)\n",
    "\n",
    "    nsplines = basis.shape[1]\n",
    "    vs = np.zeros(npts_crust+npts_mantle)\n",
    "\n",
    "    for i in range(0,nsplines):\n",
    "        vs[0:npts_crust] += basis[:,i] * coefs[i]\n",
    "\n",
    "    vs[npts_crust:] = vs_mantle\n",
    "\n",
    "    thickness = np.diff(depth)[0]\n",
    "    vs_layers = vs[0:len(vs)-1]\n",
    "    vel_model = np.zeros((len(depth)-1,4))\n",
    "            \n",
    "    vel_model[:,0] = thickness              \n",
    "    vel_model[:,1] = vs_layers * np.sqrt(3)  #Vp (km/s)\n",
    "    vel_model[:,2] = vs_layers               #Vs (km/s)\n",
    "    vel_model[:,3] = vs_layers / np.sqrt(2)  #density (gm/cm3)\n",
    "    \n",
    "    vel_model[-1,0] = 100.0 #extend the mantle thickness by 50 km.\n",
    "    \n",
    "    return vel_model\n",
    "\n",
    "def run_mcmc(per_observed,vel_observed,iterations,burn_frac=0.2,sigma=0.1):\n",
    "    '''\n",
    "    -------------------------------------------------------------------\n",
    "    input parameters:\n",
    "    -------------------------------------------------------------------\n",
    "    per_observed = an array of periods at which group velocity measurements were made (in s)\n",
    "    vel_observed = an arary of group velocity measurements, made at each perion in 'per_observed'\n",
    "    iterations = number of iterations to run the inversion for\n",
    "    burn_frac = the percentage of the total iterations of the 'burn-in' period (these interations are thrown out)\n",
    "    sigma = data error (defaults = 0.1 km/s)\n",
    "    \n",
    "    -------------------------------------------------------------------\n",
    "    returns:\n",
    "    -------------------------------------------------------------------\n",
    "    accepted = a list of accepted models\n",
    "    misfits = a list of the misfits corresponding to the models in 'accepted'\n",
    "    best_model = the model with the lowest misfit\n",
    "    '''\n",
    "    #Set the priors on the model parameters. There are a total of 7 parameters. \n",
    "    #5 parameters describing Vs in the crust, a moho depth, and the Vs of the mantle (approximated as a single layer).\n",
    "    p_low = [2.4,2.4,2.8,3.2,3.2]\n",
    "    p_high = [3.2,3.2,3.6,4.0,4.0]\n",
    "    moho_low = 10.0\n",
    "    moho_high = 50.0\n",
    "    vs_mantle_high = 4.8\n",
    "    n_skipped = 0\n",
    "    \n",
    "    #initialize empty chain\n",
    "    nchain = 0\n",
    "    accepted = []\n",
    "    misfits = []\n",
    "    misfit_lowest = np.inf\n",
    "    nburn = int(iterations*burn_frac)\n",
    "    \n",
    "    t = np.logspace(0.0,5.0,100)\n",
    "    \n",
    "    #run through the iterations\n",
    "    for i in range(0,iterations):\n",
    "\n",
    "        #randomly draw model parameters from uniform priors.  \n",
    "        m0 = np.random.uniform(p_low[0],p_high[0])\n",
    "        m1 = np.random.uniform(p_low[1],p_high[1])\n",
    "        m2 = np.random.uniform(p_low[2],p_high[2])\n",
    "        m3 = np.random.uniform(p_low[3],p_high[3])\n",
    "        m4 = np.random.uniform(p_low[4],p_high[4])\n",
    "        coefs = [m0,m1,m2,m3,m4]\n",
    "        moho = np.random.uniform(moho_low,moho_high)\n",
    "        vs_mantle = np.random.uniform(m4,vs_mantle_high)\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print('iteration {}'.format(i))\n",
    "            \n",
    "        #run forward problem (ie. calculate a dispersion curve with disba)\n",
    "        vel_model = gen_model(moho=moho,coefs=coefs,vs_mantle=vs_mantle)\n",
    "        gd = GroupDispersion(*vel_model.T)\n",
    "        \n",
    "        try:\n",
    "            gdr = gd(t,mode=0,wave='rayleigh')\n",
    "        except disba.DispersionError:\n",
    "            n_skipped += 1 #sometimes the forward calculation fails... just skip in this case\n",
    "            continue\n",
    "\n",
    "        per_modeled = gdr[0]\n",
    "        vel_modeled = gdr[1]\n",
    "        vel_modeled = np.interp(per_observed,per_modeled,vel_modeled) #interpolate modeled values\n",
    "        \n",
    "        #calculate misfit and likelihood function\n",
    "        if i == 0:\n",
    "            misfit0 = np.linalg.norm(((vel_observed - vel_modeled) / sigma)**2)\n",
    "            phi0 = np.exp(-misfit0/2)\n",
    "        else:\n",
    "            misfit = np.linalg.norm(((vel_observed - vel_modeled) / sigma)**2)\n",
    "            phi = np.exp(-misfit/2)\n",
    "            r = phi / phi0\n",
    "            \n",
    "            if misfit < misfit_lowest and i > nburn:\n",
    "                misfit_lowest = misfit\n",
    "                best_model = [coefs,moho,vs_mantle]\n",
    "                #print('misfit_lowest {}, best_model {}'.format(misfit_lowest,best_model))\n",
    "\n",
    "            #roll a random number\n",
    "            roll = np.random.uniform(0,1)\n",
    "\n",
    "            if r > roll and i > nburn:\n",
    "                nchain += 1\n",
    "                accepted.append([coefs,moho,vs_mantle])\n",
    "                misfits.append(misfit)\n",
    "                \n",
    "    print('{} skipped'.format(n_skipped))\n",
    "    return accepted,misfits,best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8007468e",
   "metadata": {},
   "source": [
    "### <font color='red'>Question 5 </font> \n",
    "\n",
    "Run the function `run_mcmc` to invert your measured group velocity dispersion curve, setting the number of iterations to 5000. The inversion will likely take several minutes. The function will print the current iteration number every 100 iterations, so you can monitor progress. If is taking a long time to run the inversion (e.g., it takes several minutes to perform 100 iterations), something is wrong and you should stop the process. Please let me know if this happens so I can help.\n",
    "\n",
    "In general, the more iterations you run for, the more accurately your chain of accepted models will represent the true posterior probability distribution (until you get to a point of convergence). So you can try running the inversion with a different number of iterations and comparing the results, but you shouldn't perform too many iterations. I would avoid running it for many more than 10,000 because it will start to take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a2a5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer Question 5 here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f761d0dc",
   "metadata": {},
   "source": [
    "### Analyzing the results\n",
    "\n",
    "Let's take a look at the inversion results! The function below, `plot_results`, will make a plot to summarize the inversion. The function takes 5 input arguments. The first three arguments (`accepted`, `misfits`, and `best_model`) are what was returned by `run_mcmc`. The remaining two arguments (`per_observed`, and `vel_observed`) describe the observed despersion curve (i.e., the periods and group velocity measurements, respectively). \n",
    "\n",
    "**Note:** Here we only plot the average of the best 100 models from the 'ensemble' of solutions that was returned by the MCMC inversion. In principle, the ensemble of models can be used to approximate the posterior probability distribution $p(m|d)$, which gives us information on the uncertainty of model parameters. To make things simple, we won't consider the model uncertainties, and only work with the mean model, and 'best' model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aa0d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(accepted, misfits, best_model, per_observed, vel_observed):\n",
    "    '''\n",
    "    plots a summary of the results\n",
    "    '''\n",
    "    \n",
    "    fig,axes = plt.subplots(figsize=[10,5],ncols=2)\n",
    "    \n",
    "    #Generate 'best' model\n",
    "    coefs_best = best_model[0]\n",
    "    moho_best = best_model[1]\n",
    "    vs_mantle_best = best_model[2]\n",
    "    vel_best = gen_model(moho=moho_best,coefs=coefs_best,vs_mantle=vs_mantle_best)\n",
    "    \n",
    "    #Generate model from ensemble average (only use best N models): this is sort of cheating\n",
    "    n_best = 100\n",
    "    inds = np.argsort(misfits)\n",
    "\n",
    "    accepted_nbest = []\n",
    "    for i in range(0,n_best):\n",
    "        accepted_nbest.append(accepted[inds[i]])\n",
    "\n",
    "    coefs_accepted = 0\n",
    "    moho_accepted = 0\n",
    "    vs_mantle_accepted = 0\n",
    "\n",
    "    for i in range(0,len(accepted_nbest)):\n",
    "        coefs_accepted += np.array(accepted_nbest[i][0])\n",
    "        moho_accepted += accepted_nbest[i][1]\n",
    "        vs_mantle_accepted += accepted_nbest[i][2]\n",
    "\n",
    "    ave_coefs = coefs_accepted/n_best\n",
    "    ave_moho = moho_accepted/n_best\n",
    "    ave_vs_mantle = vs_mantle_accepted/n_best\n",
    "    vel_avg = gen_model(moho=ave_moho,coefs=ave_coefs,vs_mantle=ave_vs_mantle)\n",
    "    \n",
    "    #-----------------------------------------------------------------------------\n",
    "    # plot models\n",
    "    #-----------------------------------------------------------------------------\n",
    "    axes[0].plot(vel_avg[:,2],np.cumsum(vel_avg[:,0]),label='mean model')\n",
    "    axes[0].plot(vel_best[:,2],np.cumsum(vel_best[:,0]),label='best model')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_xlim([1.5,5.5])\n",
    "    axes[0].set_ylim([85,0])\n",
    "    axes[0].set_xlabel('Vs (km/s)')\n",
    "    axes[0].set_ylabel('depth (km)')\n",
    "    \n",
    "    #-----------------------------------------------------------------------------\n",
    "    # plot observations vs model predictions\n",
    "    #-----------------------------------------------------------------------------\n",
    "    \n",
    "    #observations\n",
    "    axes[1].errorbar(per_observed,vel_observed,yerr=0.1,color='black',capsize=5,fmt='o',label='observed')\n",
    "\n",
    "    #plot predictions for average model\n",
    "    t = np.logspace(0.0,5.0,100)\n",
    "    gd = GroupDispersion(*vel_avg.T)\n",
    "    gdr = gd(t,mode=0,wave='rayleigh')\n",
    "    vel_pred = np.interp(per_observed,gdr[0],gdr[1])\n",
    "    axes[1].plot(per_observed,vel_pred,label='predicted, mean model',zorder=98)\n",
    "\n",
    "    #plot predictions for best model\n",
    "    gd = GroupDispersion(*vel_best.T)\n",
    "    gdr = gd(t,mode=0,wave='rayleigh')\n",
    "    vel_pred = np.interp(per_observed,gdr[0],gdr[1])\n",
    "    axes[1].plot(per_observed,vel_pred,label='predicted, best model',zorder=99)\n",
    "    \n",
    "    axes[1].legend()\n",
    "    axes[1].set_ylim([2.0,4.5])\n",
    "    axes[1].set_xlabel('period (s)')\n",
    "    axes[1].set_ylabel('group velocity (km/s)')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4885229",
   "metadata": {},
   "source": [
    "### <font color='red'>Question 6 </font> \n",
    "\n",
    "i) Run `plot_results` to generate a figure. The figure will have two panels. The left panel shows the shear wave speed as a function of depth for i) the mean of the accepted models and ii) the best fitting model in the ensemble (i.e., the model with the lowest misfit). The right panel shows the observed group velocity dispersion measurements (with error bars depicting 0.1 km/s uncertainty bounds), along with the predicted dispersion curve for the mean model and best model.\n",
    "\n",
    "ii) Describe the main features of the models, including the crustal thickness (i.e., Moho depth), and how Vs varies with depth.  \n",
    "\n",
    "iii) Do the mean model and/or best model fit the data well? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d019bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer Question 6 here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e8ca04",
   "metadata": {},
   "source": [
    "### <font color='red'>Question 7 </font> \n",
    "\n",
    "The CRUST2.0 model (https://igppweb.ucsd.edu/~gabi/crust2.html) gives a global estimate of crustal thickness at 2x2 degree resolution. Do your inversion results agree with what you would expect based on CRUST 2.0? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73481514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer Question 7 here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbe465e",
   "metadata": {},
   "source": [
    "### <font color='red'>Question 8 </font> \n",
    "\n",
    "In this analysis, we approximated the mantle as a single layer with a uniform velocity. \n",
    "\n",
    "i) Why is this a reasonable approximation? (or is it?)\n",
    "\n",
    "ii) Let's say we want to resolve the velocity structure deeper in the mantle (e.g., down to 200 km) What could we change about this analysis to accomplish this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7030bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Question 8 here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
