{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21c834dd",
   "metadata": {},
   "source": [
    "# GEOL 593: Seismology and Earth Structure\n",
    "\n",
    "## Lab assignment 8: Receiver functions\n",
    "\n",
    "In this lab we will get experience calculating reciever functions from real data recorded by a seismometer in Illinois in order to infer some properties of the crust and mantle. The dataset we will work with is located in `../data/lab_08`, and consists of 3-component broadband data recorded from teleseismic earthquakes at station N4.N41A near Monmouth in western Illinois.\n",
    "\n",
    "If you are interested in how the data was initially collected, I used the tool ObspyDMT (https://github.com/kasra-hosseini/obspyDMT), which makes it very easy to download and process bulk data for events that match your criteria. For teleseismic receiver function analysis, we are interested in events between 30 - 90 degrees epicentral distance. Typically, studies will use event magnitudes between ~ 5.5 - 7, although here we used a lower cutoff of 6.3 to limit the size of the dataset. For those curious, the data in this study was downloaded with ObspyDMT using the command line argument\n",
    "\n",
    "`obspyDMT --datapath N41A_10hz --min_date 2014-01-01 --max_date 2023-01-01 --min_mag 6.3 --max_mag 7.0 --min_epi 30 --max_epi 90 --event_catalog IRIS --net N4 --sta N41A --cha HH* --preset 30 --offset 120 --instrument_correction --data_source IRIS --cut_time_phase --sampling_rate 10.0`\n",
    "\n",
    "Generally, a fair proportion of the events will not yeild useable signals, so another step of manual quality control is usually necessary. After this step, we were left with data from 92 events, which we will use for receiver function analysis. The seismic data is in MSEED format, and is located in `../data/lab_08/events`. There are 3 MSEED files (1 for each component) in each event sub-directory. The metadata of each event, including its longitude, latitude,depth,  and magnitude is located in the file `../data/lab_08/event_catalog.txt`. Run the code below to read this file, and generate lists of each event attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9860c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import obspy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.crs as ccrs\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy.geodetics import gps2dist_azimuth\n",
    "from scipy import signal\n",
    "import cmath\n",
    "\n",
    "#station info\n",
    "client = Client(\"IRIS\")\n",
    "inv = client.get_stations(network='N4',station='N41A',level='response')\n",
    "st_lon = -90.85\n",
    "st_lat = 40.70\n",
    "\n",
    "#read catalog\n",
    "catalog = open('../data/lab_08/event_catalog.txt','r')\n",
    "lines = catalog.readlines()\n",
    "ev_names = []\n",
    "ev_lats = []\n",
    "ev_lons = []\n",
    "ev_deps = []\n",
    "ev_mags = []\n",
    "ev_dists = []\n",
    "for i,line in enumerate(lines):\n",
    "    \n",
    "    #skip line 1\n",
    "    if i == 0:\n",
    "        continue\n",
    "    #otherwise, append items to list\n",
    "    else:   \n",
    "        items = line.split()\n",
    "        ev_names.append(items[0])\n",
    "        ev_lat = float(items[1])\n",
    "        ev_lon = float(items[2])\n",
    "        ev_dep = float(items[3])\n",
    "        ev_mag = float(items[4])\n",
    "        ev_lats.append(ev_lat)\n",
    "        ev_lons.append(ev_lon)\n",
    "        ev_deps.append(ev_dep)\n",
    "        ev_mags.append(ev_mag)\n",
    "        \n",
    "        #calculate distance\n",
    "        dist_m,az,baz = gps2dist_azimuth(ev_lat,ev_lon,st_lat,st_lon)\n",
    "        ev_dists.append(dist_m/1000.)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb04cee",
   "metadata": {},
   "source": [
    "###  <font color='red'>Question 1 </font> \n",
    "\n",
    "Using cartopy, make a map of the location of the seismic station and all of the events. Given the large range of epicentral distances and azimuths, an appropriate choice for the map projection is \"AzimuthalEquidistant\" (see list of possible projections here: https://scitools.org.uk/cartopy/docs/v0.15/crs/projections.html#cartopy-projections).\n",
    "\n",
    "Hint: When plotting scatter data on a global map, be sure to use the option `ax.set_global()`. Otherwise, the figure may be re-centered and only show the region of the map where there is data plotted. Also, using `ax.scatter()` requires you to set a `transform` argument. In the past, when making mercator maps, you have used `transform=ccrs.Geodetic()`. When using the AzimuthalEquidistant project, set `transform=ccrs.PlateCarree()` to ensure your data is plotted in the proper location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f6c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer Q1 here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836dfb79",
   "metadata": {},
   "source": [
    "### Calculating receiver functions\n",
    "\n",
    "Calculating radial P-to-S receiver functions requires several steps. First, you must trim the 3-component data in a window surrounding the P-wave. The data in `../data/lab_08/events` is already appropriately windowed, so you don't have to worry about this. All waveforms have been trimmed so that they begin 30 s prior to the expected P-wave arrival, and end 120-s after P. Next, you need to rotate the data into the 'RTZ' coordinate system. In this coordinate system, the P-wave is predominantly on the vertical (Z) component, and the P-to-S conversions are predominantly on the radial (R) component. The last step is to deconvolve the P-wave (approximated by the vertical component seismogram) from the radial component. The resulting time series is the radial receiver function.\n",
    "\n",
    "The deconvolution step is not trivial. There are multiple ways to perform deconvolution, and some of them are more stable than others. In class, you have seen how deconvolution can be performed as a 'spectral division' (i.e., division of two signals in the frequency domain). To avoid division by zero, this approach needs to be stabilized by a 'water-level' parameter. The water level value is a subjective choice, although there are ways for determining an optimal value. Another approach is the use time-domain deconvolution (e.g., Liggoria & Ammon, 1999), which has some advantages of traditional water-level deconvolution.\n",
    "\n",
    "Below, the function `calc_rf` is provided to calculate the radial receiver function, provided the radial and vertical component traces. The function also provides a choice of how the deconvolution is performed. You can choose `method='water_level'` to use frequency-domain water-level deconvolution, or `method='iterative_decon'` to use time-domain iterative deconvolution. In the former case, the `alpha` parameter sets the water level value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8ef084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rf(tr_r,tr_z,method='water_level',alpha=0.1):\n",
    "    '''\n",
    "    calculates the radial receiver function by deconvolving the vertical component from the radial component\n",
    "    \n",
    "    method: 'water_level' or 'iterative_decon'\n",
    "    '''\n",
    "    a = tr_r.data\n",
    "    b = tr_z.data\n",
    "    \n",
    "    if method == 'water_level':\n",
    "        rf = water_level_decon(a,b,dt=tr_z.stats.delta,wl=alpha)\n",
    "    elif method == 'iterative_decon':\n",
    "        rf = iterative_deconvolution(a,b,dt=tr_z.stats.delta)\n",
    "    \n",
    "    return rf\n",
    "\n",
    "def water_level_decon(comp2, comp1, dt, wl=0.001,):\n",
    "    '''\n",
    "    Water-level deconvolution: Modified from Sanne Cottaar's smurfpy receiver function code (https://github.com/oxalorg/smurf/blob/master/smurf.py)\n",
    "\n",
    "    Deconvolves comp2 by comp1\n",
    "    #--------------------------------------------------------\n",
    "    comp2 = numerator\n",
    "    comp1 = denominator\n",
    "    dt = 1/(sampling rate) seconds\n",
    "    wl = the water-level, usually ranges from 1e-5 to 1e-1\n",
    "    '''\n",
    "    \n",
    "    #Set some parameters\n",
    "    fmax=0.5 #cutoff frequency for filter\n",
    "    timeshift=30. #shift between beginning of RF and first arrival\n",
    "\n",
    "    # Pad the reference component\n",
    "    padded = np.zeros_like(comp2)\n",
    "    padded[0:comp1.size] = comp1\n",
    "\n",
    "    # Convert to frequency domain\n",
    "    c2freq = np.fft.fft(comp2)\n",
    "    c1freq = np.fft.fft(padded)\n",
    "    frq    = np.fft.fftfreq(len(comp2),dt)\n",
    "\n",
    "    # Construct the denominator, fill in the troughs to stabalize the deconvolution\n",
    "    wlfreq = c1freq * c1freq.conjugate()\n",
    "    wlfreq[wlfreq.real < wl*wlfreq.real.max()] = complex(wl*wlfreq.real.max(),0.) # water-level\n",
    "    \n",
    "    filterf = np.square( np.cos( np.pi*frq/(2.*fmax) ) ) # fmax is the cut-off frequency\n",
    "    filterf[np.abs(frq)>fmax] = 0.\n",
    "\n",
    "    # Perform deconvolution and filter\n",
    "    newfreq = c2freq * c1freq.conjugate() / (wlfreq)\n",
    "    newfreq = newfreq * filterf\n",
    "    # Phase shift to recover time\n",
    "    phaseshift = [cmath.exp(1j* (-frq[x]*2.*np.pi*timeshift) ) for x in range(len(frq))]\n",
    "    newfreq    = newfreq * phaseshift\n",
    "    # Convert back to time\n",
    "    RF  = np.fft.ifft((newfreq),len(comp2))\n",
    "\n",
    "    # Reconvolve with vertical component\n",
    "    conv=np.real(np.convolve(RF,padded,'full'))\n",
    "    conv=conv[int(timeshift/dt):int(timeshift/dt+len(comp2))]\n",
    "    \n",
    "    # Calculate fit\n",
    "    comp2f=np.real(np.fft.ifft(filterf*np.fft.fft(comp2)))\n",
    "    # Calculate residual and fit\n",
    "    residual=comp2f-conv\n",
    "    fit= 100.* (1.- sum(residual*residual)/sum(comp2f*comp2f))\n",
    "    decon = RF.real\n",
    "\n",
    "    return decon\n",
    "\n",
    "def iterative_deconvolution(comp2,comp1,dt):\n",
    "    '''\n",
    "    Perform iterative time domain deconvoliution (e.g., Ligorria & Ammon 1999)\n",
    "    Modified from Sanne Cottaar's smurfpy code\n",
    "    '''\n",
    "    #Set some parameters\n",
    "    fmax=0.5 #cutoff frequency for filter\n",
    "    timeshift=30. #shift between beginning of RF and first arrival\n",
    "    maxbumps=200 #maximum number of pulses to construct RF\n",
    "    \n",
    "    # Frequency domain\n",
    "    NFFT = len(comp2)\n",
    "    frq  = np.fft.fftfreq(NFFT,dt)\n",
    "\n",
    "    #cosine filter\n",
    "    filterf = np.square( np.cos( np.pi*frq/(2.*fmax) ) ) # fmax is the cut-off frequency\n",
    "    filterf[np.abs(frq)>fmax] = 0.\n",
    "\n",
    "    # Filter components\n",
    "    comp2f=np.real(np.fft.ifft(filterf*np.fft.fft(comp2)))\n",
    "    comp1f=np.real(np.fft.ifft(filterf*np.fft.fft(comp1)))\n",
    "\n",
    "    # Computer power in numerator\n",
    "    power=sum(comp1f*comp1f)\n",
    "    # Initialize values\n",
    "    maxind=np.empty([maxbumps,1],dtype=int)\n",
    "    maxvalue=np.empty([maxbumps,1],dtype=float)\n",
    "    residual=comp2f # Initial residual is horizontal component\n",
    "    fit_old=0 # Initial fit = 0%\n",
    "\n",
    "    for peak in range(maxbumps):\n",
    "    # Correlate signals and find peak\n",
    "        corr=(np.correlate(residual,comp1f,'full'))\n",
    "        corr=corr[len(comp2)-int(timeshift/dt):-int(timeshift/dt)]# Set maximum Xcorr to be at the predicted main arrival\n",
    "        maxind[peak]=np.argmax(np.absolute(corr))# Peak index\n",
    "        maxvalue[peak]=corr[maxind[peak]]/(power)# Peak amplitude\n",
    "\n",
    "        # Build deconvolution result\n",
    "        decon=np.zeros_like(comp2)\n",
    "        for ind in range(peak+1):\n",
    "            decon[maxind[ind]]=decon[maxind[ind]]+maxvalue[ind]\n",
    "\n",
    "        decon=np.real(np.fft.ifft(filterf*np.fft.fft(decon)))    # Filter deconvolution result\n",
    "\n",
    "        # Reconvolve with vertical component\n",
    "        conv=np.real(np.convolve(decon,comp1,'full'))\n",
    "        conv=conv[int(timeshift/dt):int(timeshift/dt)+len(comp2)]\n",
    "\n",
    "        # Calculate residual and fit\n",
    "        residual=comp2f-conv\n",
    "        fit= 100.* (1.- sum(residual*residual)/sum(comp2f*comp2f))\n",
    "        if (fit-fit_old)<0.01:\n",
    "            break\n",
    "        fit_old=fit\n",
    "\n",
    "    return decon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d373784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rfs_all(ev_lons,ev_lats,ev_names,method='water_level',alpha=0.005):\n",
    "\n",
    "    data_dir = '/Users/rmaguire/Desktop/geol_593/data/lab_08'\n",
    "    rf_list = []\n",
    "\n",
    "    for i in range(0,len(ev_names)):\n",
    "        #print('calculating RF for event {} ({}/{})'.format(ev_names[i],i+1,len(ev_names)))\n",
    "    \n",
    "        #read seismic data\n",
    "        st = obspy.read('{}/events/{}/*'.format(data_dir,ev_names[i]))\n",
    "    \n",
    "        #pre-processing (filter, and rotate)\n",
    "        st.taper(0.05)\n",
    "        st.detrend()\n",
    "        st.filter('bandpass',freqmin=1./5,freqmax=2.0,corners=2,zerophase=False)\n",
    "        st.rotate('->ZNE',inventory=inv) #rotate INTO ZNE (assuming some channels are BH1, BH2)\n",
    "        #dist_m,az,baz = gps2dist_azimuth(evla,evlo,stla,stlo) #calculate back-azimuth\n",
    "        dist_m,az,baz = gps2dist_azimuth(ev_lats[i],ev_lons[i],st_lat,st_lon) #calculate back-azimuth\n",
    "        st.rotate('NE->RT',back_azimuth=baz) #rotate from ZNE to RTZ\n",
    "    \n",
    "        #calculate receiver functions\n",
    "        tr_z = st.select(channel='*Z')[0]\n",
    "        tr_r = st.select(channel='*R')[0]\n",
    "        rf = calc_rf(tr_r.copy(),tr_z.copy(),method=method,alpha = alpha)\n",
    "        rf_list.append(rf)\n",
    "        \n",
    "    rfs = np.array(rf_list)\n",
    "        \n",
    "    return rfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665e1566",
   "metadata": {},
   "source": [
    "###  <font color='red'>Question 2 </font>\n",
    "\n",
    "The function above `calc_rfs_all` calculates radial receiver functions for all of the events. This function is loops through each event, performs some pre-processing, and calls the `calc_rf`. Each receiver function is added to a numpy array (named `rfs`) which has the shape of (N,M), where N is the number of receiver functions (should be 92 in this case) and M is the number of points in a single receiver function (should be 750 here, which is the same number of points in each original seismogram). The function requires the parameters `ev_lons`,`ev_lats`,and `ev_names`, which are lists that were calculated above when we read the event catalog. You must also specify the deconvolution method and, optionally the water level parameter alpha. The function returns the numpy array containing all of the receiver functions (each row of the matrix is an individual receiver function).\n",
    "\n",
    "i) use `calc_rfs_all` to calculate receiver functions for both deconvolution methods. The default water-level parameter is fine, although you can adjust this to see the influence.\n",
    "\n",
    "ii) Plot the 'stack' (i.e., the average) of all of individual receiver functions for both deconvolution methods (i.e., you will plot one stack of the receiver functions calculated with water-level deconvolition, and one stack of the receiver functions calculated with iterative time domain deconvolution. The x-axis of the plot should be in time (s) and the y-axis should be the receiver function amplitude.In order to correctly plot the time axis, you will need to create a time vector, ranging from -30 to 120, with the same number of points as the receiver function stack.\n",
    "\n",
    "*Hint, you can calculate the average of a numpy array with `np.mean(rfs,axis=0)`, where `rfs` is the array containing all the receiver functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baca182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer Q2 here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a6f814",
   "metadata": {},
   "source": [
    "###  <font color='red'>Question 3</font> \n",
    "\n",
    "The main peak (at t = 0) corresponds to the P-wave arrival. Subsuquent pulses may indicate either P-to-S conversions below the station, or multiply reflected phases in the crust. In your stack, you should observe a clear upward pulse immediately after the P-wave arrival. This corresponds to the P-to-S conversion at the Moho (called the Pms phase). The second noticeable upward pulse (near 15-s) is the crustal multiple PpPs.\n",
    "\n",
    "i) What is the travel time of the P-to-S conversion from the Moho, relative to P? In other words, what is the peak-to-peak time difference between the direct P-wave and the Pms conversion? To answer accurately, you may want to zoom in on the plot you made above with the `ax.set_xlim()` function (e.g., `ax.set_xlim([-10,40])`).\n",
    "\n",
    "ii) Assuming an average Vp and Vs in the crust of 6.3 and 3.5 km/s, respectively, what is the approximate thickness of the crust below this station?\n",
    "\n",
    "*Hint* The equation for the crustal thickness, assuming the P and P-to-S conversion are traveling vertically through the crust (a reasonably accuarate assumption) is\n",
    "\n",
    "$\n",
    "\\displaystyle\n",
    "H = \\frac{\\Delta t}{\\frac{1}{V_S}-\\frac{1}{V_P}}\n",
    "$\n",
    "\n",
    "where $H$ is the crustal thickness and $\\Delta t$ is the travel time delay of the P-to-S conversion at the Moho (i.e., the value you found in part i ).\n",
    "\n",
    "iii) How does your value of crustal thickness found for this station in western Illinois compare to values found in the literature for this region? For example, compare your crustal thickness estimate to the global model CRUST2.0, or the North American model of Shen & Ritzwoller (2016). (Figure 13 in this paper: https://agupubs.onlinelibrary.wiley.com/doi/full/10.1002/2016JB012887)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b80ed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer Q3 here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6835c69",
   "metadata": {},
   "source": [
    "### Transition zone receiver functions\n",
    "\n",
    "Radial receiver functions are not only useful for imaging the crust-mantle boundary. The have also found extensive application in imaging the seismic discontinuties associated with mineral phase transitions in the mantle transition zone. The two predominant discontinuities in the mantle transition zone occur at approximately 410-km depth (associated with the olivine to wadsleyite mineral phase change) and 660-km depth (associated with the ringwoodite to bridgmanite + ferropericlase transition). These seismically observable discontinuities are often referred to as 'the 410' and 'the 660', respectively, even though the true depths to these boundaries may vary regionally from the nominal depths.\n",
    "\n",
    "Similarly to the analysis above for imaging the Moho, identifying clear P-to-S conversions from the 410 and the 660 often requires stacking receiver functions for multiple events. However, for these deeper conversions, we need to make an additional consideration. This is because the predicted travel times of deeper P-to-S conversions depend more strongly on the epicentral distance of the event. The dependence of the phase arrival time on the distance is referred to as the 'moveout'. Therefore, before we stack signals from different events, we must account for the moveout. One way to do this is using a 'delay and sum' approach, where individual receiver functions are shifted in time by some amount prior to stacking. The amount which they are shifted is determined by the epicentral distance, and a reference distance (here we use 64 degrees). \n",
    "\n",
    "The function below `delay_and_sum` is provided to make a 'moveout corrected stack'. The function takes a numpy array of receiver functions `rfs`, the lists of earthquake depths and distances(`ev_deps` and `ev_dists`), and the name of the phase for which to calculate the moveout (e.g., 'P410s' for the P-to-S conversion at 410-km depth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10d84cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.taup import TauPyModel\n",
    "from obspy.geodetics import kilometer2degrees\n",
    "def delay_and_sum(rfs,ev_deps,ev_dists,phase,ref_dist=64.0):\n",
    "    '''\n",
    "    Shifts the signal to account for moveout of a given phase\n",
    "\n",
    "    inputs---------------------------------------------------------------------\n",
    "    rfs: array of receiver functions\n",
    "    ev_deps: list of event depths\n",
    "    ev_dists: list of event distances (assumes in km)\n",
    "    phase:   phase name for which to apply moveout correction (\"Pms\", \"P410s\", \"P660s\")\n",
    "    ref_dist: float, reference epicentral distance\n",
    "\n",
    "    '''\n",
    "    model = TauPyModel('PREM')\n",
    "    rfs_new = np.zeros(rfs.shape)\n",
    "    for i in range(0,len(ev_lons)):\n",
    "        t_ref = model.get_travel_times(source_depth_in_km=ev_deps[i],\n",
    "                                       distance_in_degree=ref_dist,\n",
    "                                       phase_list=[\"P\",phase])\n",
    "        t_arr = model.get_travel_times(source_depth_in_km=ev_deps[i],\n",
    "                                       distance_in_degree=kilometer2degrees(ev_dists[i]),\n",
    "                                       phase_list=[\"P\",phase])\n",
    "        for arr in t_ref:\n",
    "            if arr.name=='P':\n",
    "                p_arrival_ref = arr.time\n",
    "            elif arr.name==phase:\n",
    "                pds_arrival_ref = arr.time\n",
    "\n",
    "        for arr in t_arr:\n",
    "            if arr.name=='P':\n",
    "                p_arrival = arr.time\n",
    "            elif arr.name==phase:\n",
    "                pds_arrival = arr.time\n",
    "\n",
    "        time_shift = (pds_arrival_ref-p_arrival_ref)-(pds_arrival-p_arrival)\n",
    "        int_shift  = int(time_shift/0.2) #0.2 is 1./samping_rate of this data\n",
    "        data = rfs[i,:]\n",
    "        data *= signal.tukey(len(data)) #apply a tapered window (avoid edge artifacts)\n",
    "        rfs_new[i,:] = np.roll(data,int_shift)\n",
    "    \n",
    "    rf_stack = np.mean(rfs_new,axis=0)\n",
    "    return rf_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61ac382",
   "metadata": {},
   "source": [
    "###  <font color='red'>Question 4</font> \n",
    "\n",
    "i)Use the function `delay_and_sum` to calculate moveout corrected receiver function stacks based on 3 different phases: Pms (the moho conversion), P410s (the 410 conversion) and P660s (the 660 conversion). You can use the receiver functions calculated with either the water level or the time domain deconvolution methods. The time domain results tend to be a little cleaner. Using the same axes, plot all three of the stacks. Be sure to use labels and a figure legend.\n",
    "\n",
    "ii) A P-to-S conversion should maximize in amplitude if the proper moveout correction is applied. For example, if the moveout correction is performed for the P410s phase, the P410s signal should be maximized in the stack, and would be diminished if the moveout correction was applied for a different phase. At approximately what times do you notice signals 'maximizing' when performing the moveout corrections for P410s and P660s? Only one peak (corresponding to the P410s or P660s) should maximize. Focus on signals after ~20 s. We already know that the first two prominent peaks following P are the Pms and PpPs phase.\n",
    "\n",
    "iii) PREM predicts a P660s - P410s time of 26.3 s. How does this compare to your observations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12478325",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer Q4 here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d4e62c",
   "metadata": {},
   "source": [
    "###  <font color='red'>Question 5</font> \n",
    "\n",
    "If the travel time difference $P660s - P410s$ is smaller than average, what could this imply about the transition zone structure? Think in terms of temperature and/or thickness of the transition zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6b8829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer Q5 here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
